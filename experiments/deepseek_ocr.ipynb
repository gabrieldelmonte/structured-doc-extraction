{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek OCR - Experimento com modelo quantizado (4-bit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4jKQq9uEdsY",
        "outputId": "062899ac-8ba6-47e5-f043-9a422a33effd"
      },
      "source": [
        "Este notebook demonstra o uso do modelo **DeepSeek-OCR** quantizado em 4-bit para extração de texto de imagens\n",
        "\n",
        "## Sobre o experimento\n",
        "- **Modelo**: DeepSeek-OCR quantizado em 4-bit (NF4)\n",
        "- **Objetivo**: Realizar OCR (Optical Character Recognition) em imagens e documentos\n",
        "- **Ambiente**: Google Colab com GPU T4\n",
        "\n",
        "## Configuração inicial\n",
        "Primeiro, vamos instalar todas as dependências necessárias usando o `uv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install uv package manager\n",
        "!pip install -q uv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalação das dependências principais\n",
        "\n",
        "Agora vamos instalar todas as bibliotecas necessárias:\n",
        "- **PyTorch**: Framework de deep learning\n",
        "- **Transformers**: Biblioteca da Hugging Face para modelos de linguagem\n",
        "- **BitsAndBytes**: Para quantização de modelos\n",
        "- **Accelerate**: Para otimização de inferência\n",
        "- **Pillow**: Para processamento de imagens\n",
        "- Outras dependências auxiliares\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies with specific versions\n",
        "!uv pip install torch==2.6.0 transformers==4.46.3 tokenizers==0.20.3 einops addict easydict pillow torchvision bitsandbytes accelerate numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalação do Flash Attention (Opcional)\n",
        "\n",
        "**Importante**: O `flash-attn` requer GPUs modernas com suporte a CUDA compute capability >= 8.0\n",
        "\n",
        "**GPUs compatíveis:**\n",
        "- NVIDIA A100, A10, A6000\n",
        "- RTX 30xx series (3090, 3080, etc.)\n",
        "- RTX 40xx series\n",
        "\n",
        "**GPUs NÃO compatíveis:**\n",
        "- Tesla T4 (Google Colab) - compute capability 7.5\n",
        "- GPUs mais antigas\n",
        "\n",
        "Como estamos usando o ambiente do Google Colab com GPU T4, **não poderemos usar** `flash_attention_2`, para isso, usaremos a implementação `eager` no lugar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the line below if you have a compatible GPU (compute capability >= 8.0)\n",
        "# !uv pip install flash-attn==2.7.3 --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importação das bibliotecas\n",
        "\n",
        "Agora vamos importar todas as bibliotecas necessárias para o experimento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregando o modelo\n",
        "\n",
        "Vamos carregar o modelo **DeepSeek-OCR** quantizado em 4-bit\n",
        "\n",
        "A quantização reduz significativamente o uso de memória, permitindo rodar o modelo em GPUs com menos VRAM e ainda sim obter bons resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set CUDA device\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# Model configuration\n",
        "model_name = 'Jalea96/DeepSeek-OCR-bnb-4bit-NF4'    # 4-bit quantized model\n",
        "\n",
        "# If you want to use the non-quantized model, uncomment the line below and comment the line above\n",
        "# model_name = 'deepseek-ai/DeepSeek-OCR'            # Non-quantized model\n",
        "\n",
        "print('=' * 70)\n",
        "print('CARREGANDO MODELO DEEPSEEK-OCR')\n",
        "print(f'\\nModelo: {model_name}')\n",
        "print('=' * 70)\n",
        "\n",
        "print('\\nCarregando tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code = True\n",
        ")\n",
        "print('Tokenizer carregado com sucesso!')\n",
        "\n",
        "print('\\nCarregando modelo...')\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_name,\n",
        "    _attn_implementation = 'eager',   # Using 'eager' because T4 GPU doesn't support flash_attention_2\n",
        "    trust_remote_code = True,\n",
        "    use_safetensors = True,\n",
        "    device_map = 'auto',\n",
        "    torch_dtype = torch.bfloat16,\n",
        ")\n",
        "model = model.eval()\n",
        "print('Modelo carregado com sucesso!')\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print(f'Dispositivo: {next(model.parameters()).device}')\n",
        "print(f'Tipo de dados: {next(model.parameters()).dtype}')\n",
        "print('=' * 70 + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração dos parâmetros\n",
        "\n",
        "Vamos configurar os parâmetros para o processamento de OCR:\n",
        "\n",
        "### Modos de configuração disponíveis:\n",
        "\n",
        "| Modo          | base_size | image_size    | crop_mode | Uso Recomendado                               |\n",
        "|---------------|-----------|---------------|-----------|-----------------------------------------------|\n",
        "| **Tiny**      | 512       | 512           | False     | Testes rápidos, documentos simples            |\n",
        "| **Small**     | 640       | 640           | False     | Documentos de tamanho médio                   |\n",
        "| **Base**      | 1024      | 1024          | False     | Documentos padrão                             |\n",
        "| **Large**     | 1280      | 1280          | False     | Documentos complexos, alta qualidade          |\n",
        "| **Gundam**    | 1024      | 640           | True      | **Recomendado para a maioria dos documentos** |\n",
        "\n",
        "Neste experimento, vamos usar o modo **Large** para obter a melhor qualidade possível\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OCR configuration\n",
        "prompt = '<image>\\nFree OCR. '\n",
        "\n",
        "# File path\n",
        "base_path = ''\n",
        "image_file = os.path.join(base_path, 'data_experiments', 'case_01_drivers_license.jpeg')\n",
        "output_path = os.path.join(base_path, 'output')\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Model inference configuration\n",
        "base_size = 1280\n",
        "image_size = 1280\n",
        "crop_mode = False\n",
        "\n",
        "print('CONFIGURAÇÃO DO EXPERIMENTO')\n",
        "print('=' * 70)\n",
        "print(f'Arquivo de entrada: {image_file}')\n",
        "print(f'Diretório de saída: {output_path}')\n",
        "print(f'\\nParâmetros do modelo:')\n",
        "print(f'   • Modo: Large')\n",
        "print(f'   • Base size: {base_size}px')\n",
        "print(f'   • Image size: {image_size}px')\n",
        "print(f'   • Crop mode: {crop_mode}')\n",
        "print('=' * 70 + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processamento de OCR\n",
        "\n",
        "Agora vamos processar a imagem e extrair o texto usando o modelo DeepSeek-OCR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '=' * 70)\n",
        "print('INICIANDO PROCESSAMENTO DE OCR')\n",
        "print('=' * 70 + '\\n')\n",
        "\n",
        "# Check if image file exists\n",
        "if not os.path.exists(image_file):\n",
        "    print(f'Erro: arquivo de imagem não encontrado: {image_file}')\n",
        "else:\n",
        "    print(f'Processando imagem: {os.path.basename(image_file)}')\n",
        "    print('-' * 70)\n",
        "\n",
        "    try:\n",
        "        # Load and save the image\n",
        "        temp_path = os.path.join(output_path, f'temp_case_01.png')\n",
        "        Image.open(image_file).convert('RGB').save(temp_path)\n",
        "\n",
        "        print('\\nExecutando inferência do modelo...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run OCR inference\n",
        "        result = model.infer(\n",
        "            tokenizer,\n",
        "            prompt = prompt,\n",
        "            image_file = temp_path,\n",
        "            output_path = output_path,\n",
        "            base_size = base_size,\n",
        "            image_size = image_size,\n",
        "            crop_mode = crop_mode,\n",
        "            save_results = True,\n",
        "            test_compress = False\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "\n",
        "        print(f'Inferência concluída!')\n",
        "        print(f'Tempo de processamento: {inference_time:.2f} segundos')\n",
        "        print(f'Resultados salvos em: {output_path}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'\\nErro ao processar {os.path.basename(image_file)}: {str(e)}')\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('PROCESSAMENTO CONCLUÍDO!')\n",
        "print('=' * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Informações adicionais\n",
        "\n",
        "### Sobre o modelo quantizado\n",
        "\n",
        "O modelo utilizado (`Jalea96/DeepSeek-OCR-bnb-4bit-NF4`) é uma versão quantizada em 4-bit do DeepSeek-OCR original (`deepseek-ai/DeepSeek-OCR`). A quantização:\n",
        "\n",
        "- **Reduz o uso de memória** em aproximadamente 75%\n",
        "- **Permite execução em GPUs com menos VRAM** (como a T4 do Colab)\n",
        "- **Acelera a inferência** em alguns casos\n",
        "- **Pode ter pequena perda de precisão** em comparação com o modelo completo\n",
        "\n",
        "### Implementação de camada de atenção\n",
        "\n",
        "**Flash Attention 2** vs **Eager**:\n",
        "\n",
        "- `flash_attention_2`: Otimizado para GPUs modernas (compute capability >= 8.0)\n",
        "  - Mais rápido e eficiente em memória\n",
        "  - Não suportado pela GPU T4 do Google Colab\n",
        "  \n",
        "- `eager`: Implementação padrão do PyTorch\n",
        "  - Compatível com todas as GPUs\n",
        "  - Mais lento que flash attention em GPUs compatíveis\n",
        "\n",
        "### Arquivos de saída\n",
        "\n",
        "Os seguintes arquivos são gerados no diretório `output/`:\n",
        "- Arquivo de texto com o OCR extraído\n",
        "- Arquivo temporário da imagem processada\n",
        "- Possíveis visualizações intermediárias (dependendo da configuração)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_experiments",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
