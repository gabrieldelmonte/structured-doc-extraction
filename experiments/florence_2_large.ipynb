{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdecb14",
   "metadata": {},
   "source": [
    "# Florence-2 Large - Experimento com modelo de visão da Microsoft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe6c57",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb5897",
   "metadata": {},
   "source": [
    "Este notebook demonstra o uso do modelo **Florence-2 Large** da Microsoft para extração de texto de imagens através de OCR\n",
    "\n",
    "## Sobre o experimento\n",
    "\n",
    "- **Modelo**: microsoft/Florence-2-large\n",
    "- **Objetivo**: Realizar OCR (Optical Character Recognition) em imagens e documentos\n",
    "- **Ambiente**: Google Colab com GPU T4\n",
    "\n",
    "## Configuração inicial\n",
    "\n",
    "Primeiro, vamos instalar todas as dependências necessárias usando o `uv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17aeb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv package manager\n",
    "!pip install -q uv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b3cf0",
   "metadata": {},
   "source": [
    "### Instalação das dependências principais\n",
    "\n",
    "Agora vamos instalar todas as bibliotecas necessárias:\n",
    "- **PyTorch**: Framework de deep learning\n",
    "- **Transformers**: Biblioteca da Hugging Face para modelos de linguagem\n",
    "- **Pillow**: Para processamento de imagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies with specific versions\n",
    "!uv pip install transformers==4.49.0 pillow torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd22052",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7d7b7",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas\n",
    "\n",
    "Agora vamos importar todas as bibliotecas necessárias para o experimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37058e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5577062",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b04263",
   "metadata": {},
   "source": [
    "## Definição da função de inferência\n",
    "\n",
    "Esta função processa a imagem e executa a tarefa de OCR usando o modelo Florence-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3066cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(processor, model, task_prompt, image_file):\n",
    "    '''\n",
    "    Executa a inferencia do modelo Florence-2 para uma determinada tarefa.\n",
    "    \n",
    "    Args:\n",
    "        processor: Processador do modelo Florence-2\n",
    "        model: Modelo Florence-2\n",
    "        task_prompt: Prompt da tarefa (ex: '<OCR>')\n",
    "        image_file: Objeto PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        Resultado parseado da inferencia\n",
    "    '''\n",
    "\n",
    "    inputs = processor(\n",
    "        text = task_prompt,\n",
    "        images = image_file,\n",
    "        return_tensors = 'pt'\n",
    "    ).to('cuda', torch.float16)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids = inputs['input_ids'].cuda(),\n",
    "        pixel_values = inputs['pixel_values'].cuda(),\n",
    "        max_new_tokens = 1024,\n",
    "        early_stopping = False,\n",
    "        do_sample = False,\n",
    "        num_beams = 3,\n",
    "    )\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens = False)[0]\n",
    "\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task = task_prompt,\n",
    "        image_size = (image_file.width, image_file.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6f195",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f15d5a",
   "metadata": {},
   "source": [
    "## Carregando o modelo\n",
    "\n",
    "Vamos carregar o modelo **Florence-2 Large** da Microsoft e o processador correspondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID: Florence-2 Large\n",
    "model_id = 'microsoft/Florence-2-large'\n",
    "\n",
    "# If you want to use the fine-tuned model, uncomment the following line\n",
    "# model_id = 'microsoft/Florence-2-large-ft'\n",
    "\n",
    "print('=' * 70)\n",
    "print('CARREGANDO MODELO FLORENCE-2 LARGE')\n",
    "print('=' * 70)\n",
    "print(f'\\nModelo: {model_id}')\n",
    "\n",
    "print('\\nCarregando modelo...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True,\n",
    "    torch_dtype = 'auto'\n",
    ").eval().cuda()\n",
    "print('Modelo carregado com sucesso!')\n",
    "\n",
    "print('\\nCarregando processador...')\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "print('Processador carregado com sucesso!')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'Dispositivo: {next(model.parameters()).device}')\n",
    "print(f'Tipo de dados: {next(model.parameters()).dtype}')\n",
    "print('=' * 70 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b2053",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d41aa",
   "metadata": {},
   "source": [
    "## Configuração dos parâmetros\n",
    "\n",
    "Vamos configurar os caminhos das imagens e os parâmetros para o processamento de OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f36864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task prompt for OCR\n",
    "task_prompt = '<OCR>'\n",
    "\n",
    "# File paths\n",
    "base_path = ''\n",
    "image_file = os.path.join(base_path, 'data_experiments', 'case_01_drivers_license.jpeg')\n",
    "\n",
    "print('CONFIGURAÇÃO DO EXPERIMENTO')\n",
    "print('=' * 70)\n",
    "print(f'Tarefa: {task_prompt}')\n",
    "print(f'\\nArquivo de entrada: {image_file}')\n",
    "print('=' * 70 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344337c",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a276d8c",
   "metadata": {},
   "source": [
    "## Processamento de OCR\n",
    "\n",
    "Agora vamos processar a imagem e extrair o texto usando o modelo Florence-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 70)\n",
    "print('INICIANDO PROCESSAMENTO DE OCR')\n",
    "print('=' * 70 + '\\n')\n",
    "\n",
    "# Process the image\n",
    "if not os.path.exists(image_file):\n",
    "    print(f'Erro: arquivo de imagem não encontrado: {image_file}')\n",
    "else:\n",
    "    print(f'Processando imagem: {os.path.basename(image_file)}')\n",
    "    print('-' * 70)\n",
    "    \n",
    "    try:\n",
    "        # Load the image\n",
    "        image_file = Image.open(image_file).convert('RGB')\n",
    "\n",
    "        print('\\nExecutando inferência do modelo...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run OCR inference\n",
    "        result = run_example(processor, model, task_prompt, image_file)\n",
    "\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "\n",
    "        print(f'Inferência concluída!')\n",
    "        print(f'Tempo de processamento: {inference_time:.2f} segundos')\n",
    "\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('TEXTO EXTRAÍDO')\n",
    "        print('=' * 70)\n",
    "        print(result)\n",
    "        print('=' * 70)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'\\nErro ao processar {os.path.basename(image_file)}: {str(e)}')\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('PROCESSAMENTO CONCLUÍDO!')\n",
    "print('=' * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf835a9a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f614cd",
   "metadata": {},
   "source": [
    "## Informações adicionais\n",
    "\n",
    "### Sobre o Modelo Florence-2\n",
    "\n",
    "O **Florence-2** é um modelo de visão e linguagem da Microsoft capaz de realizar múltiplas tarefas de visão computacional, incluindo:\n",
    "\n",
    "- OCR (Optical Character Recognition)\n",
    "- Detecção de objetos\n",
    "- Segmentação\n",
    "- Captioning de imagens\n",
    "- Visual Question Answering\n",
    "\n",
    "### Prompts Disponíveis\n",
    "\n",
    "O modelo Florence-2 suporta diversos prompts de tarefa. Para OCR, o prompt deve ser exatamente:\n",
    "\n",
    "```\n",
    "<OCR>\n",
    "```\n",
    "\n",
    "Outros prompts disponiveis (para diferentes tarefas):\n",
    "- `<CAPTION>`: gerar descrição da imagem\n",
    "- `<DETAILED_CAPTION>`: gerar descrição detalhada\n",
    "- `<MORE_DETAILED_CAPTION>`: gerar descrição muito detalhada\n",
    "- `<CAPTION_TO_PHRASE_GROUNDING>`: captioning para ancoragem de frases (para esta tarefa, é necessário fornecer um contexto adicional para o modelo)\n",
    "- `<OD>`: object detection\n",
    "- `<DENSE_REGION_CAPTION>`: captioning denso de regiões\n",
    "- `<REGION_PROPOSAL>`: propostas de regiões\n",
    "- `<OCR_WITH_REGION>`: OCR com regiões específicas\n",
    "\n",
    "### Configuração do modelo\n",
    "\n",
    "- **max_new_tokens**: 1024 - número máximo de tokens gerados\n",
    "- **num_beams**: 3 - Beam search para melhor qualidade\n",
    "- **early_stopping**: False - Continua geração até o fim\n",
    "- **do_sample**: False - Geração determinística\n",
    "\n",
    "### Requisitos recomendados de Hardware\n",
    "\n",
    "- GPU NVIDIA com CUDA\n",
    "- Aproximadamente 8-12GB de VRAM para o modelo completo\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
